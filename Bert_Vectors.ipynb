{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: boto3 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.10.5)\n",
      "Requirement already satisfied: requests in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.22.0)\n",
      "Requirement already satisfied: regex in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2019.8.19)\n",
      "Requirement already satisfied: numpy in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.17.3)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.3.0)\n",
      "Requirement already satisfied: tqdm in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.36.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.5 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.13.5)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2018.1.18)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.5->boto3->pytorch-pretrained-bert) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.5->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/shreyas/anaconda3/envs/bert/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.5->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape = (566, 3) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768744566203768832</td>\n",
       "      <td>earthquake italy 267 dead hundreds injured</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768822900812046340</td>\n",
       "      <td>dlusvideonews montepulciano damages earthquake...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768848577225428992</td>\n",
       "      <td>italy quake young girl found alive rubble leas...</td>\n",
       "      <td>[0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>769138472636469248</td>\n",
       "      <td>new post national disaster italy quake death t...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>769113679073800193</td>\n",
       "      <td>number dead italy quake climbs first funerals ...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  idx                                             tweets  \\\n",
       "0  768744566203768832        earthquake italy 267 dead hundreds injured    \n",
       "1  768822900812046340  dlusvideonews montepulciano damages earthquake...   \n",
       "2  768848577225428992  italy quake young girl found alive rubble leas...   \n",
       "3  769138472636469248  new post national disaster italy quake death t...   \n",
       "4  769113679073800193  number dead italy quake climbs first funerals ...   \n",
       "\n",
       "   labels  \n",
       "0     [2]  \n",
       "1     [2]  \n",
       "2  [0, 3]  \n",
       "3     [2]  \n",
       "4     [2]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from os.path import join, exists\n",
    "from collections import OrderedDict\n",
    "\n",
    "def load_json(filename: str,filepath: str = '',date_time_tag: str = '',ext: str = \".json\",\n",
    "              show_path: bool = False) -> OrderedDict:\n",
    "    file_loc = join(filepath,date_time_tag + filename + ext)\n",
    "    if show_path:\n",
    "        print(\"Reading JSON file: [{}]\".format(file_loc))\n",
    "    if exists(join(filepath,date_time_tag + filename + ext)):\n",
    "        try:\n",
    "            with open(file_loc, encoding=\"utf-8\") as file:\n",
    "                json_dict = json.load(file)\n",
    "                json_dict = OrderedDict(json_dict)\n",
    "                # json_dict = OrderedDict(json.load(file))\n",
    "            file.close()\n",
    "            return json_dict\n",
    "        except Exception as e:\n",
    "            print(\"Could not open file as JSON: [{}]. \\n Reason:[{}]\".format(file_loc,e))\n",
    "            with open(file_loc, encoding=\"utf-8\") as file:\n",
    "                json_dict = str(file)\n",
    "                json_dict = json.loads(json_dict)\n",
    "                # json_dict = OrderedDict(json_dict)\n",
    "            return json_dict\n",
    "    else:\n",
    "        print(\"File does not exist at: [{}]\".format(file_loc))\n",
    "        return False\n",
    "\n",
    "def json2df(filename = \"smerp_labeled_validation\",dataset_dir=\"\"):\n",
    "    catid2cattxt_map = load_json(filename=filename,filepath=dataset_dir)\n",
    "    idxs, tweets, labels = [], [], []\n",
    "    for idx in catid2cattxt_map.keys():\n",
    "        idxs.append(idx)\n",
    "        tweets.append(catid2cattxt_map[idx][\"parsed_tweet\"])\n",
    "        labels.append(catid2cattxt_map[idx][\"classes\"])\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"idx\"    :idxs,\n",
    "                                    \"tweets\"   :tweets,\n",
    "                                    \"labels\":labels})\n",
    "    df = df[~df['tweets'].isna()]\n",
    "    df.to_csv(path_or_buf=join(dataset_dir,filename + \"_df.csv\"))\n",
    "    print(\"Data shape = {} \".format(df.shape))\n",
    "    return df\n",
    "\n",
    "df = json2df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "# Isnt required here as already cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_spacy(input_text: str,remove_stopwords=False):\n",
    "    input_text = spacy_en(input_text)\n",
    "    tokens = []\n",
    "    for token in input_text:\n",
    "        if remove_stopwords and token.text in STOP_WORDS:\n",
    "            continue\n",
    "        tokens.append(token.text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# all_stops = set(STOP_WORDS) | set(string.punctuation)\n",
    "def clean_txt(txt):\n",
    "    txt = re.sub('[^A-Za-z0-9 ]+', '', txt)\n",
    "    txt = [token for token in tokenizer_spacy(txt) if token.lower() not in STOP_WORDS]\n",
    "#     logger.debug(type(txt))\n",
    "#     logger.debug(txt)\n",
    "    txt = \" \".join(txt)\n",
    "#     logger.debug(type(txt))\n",
    "#     logger.debug(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             earthquake italy 267 dead hundreds injured\n",
       "1      dlusvideonews montepulciano damages earthquake...\n",
       "2      italy quake young girl found alive rubble 240 ...\n",
       "3      new post national disaster italy quake death t...\n",
       "4      number dead italy quake climbs funerals held i...\n",
       "                             ...                        \n",
       "561    urlurl canadian dead italys earthquake canadia...\n",
       "562    dead toll rises italy earthquake 120 reported ...\n",
       "563    mayor amatrice italy says quakedamaged city ne...\n",
       "564    usgs estimated significant casualties likely e...\n",
       "565    death toll italyearthquake 250 365 people inju...\n",
       "Name: tweets, Length: 566, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweets\"] = df[\"tweets\"].apply(clean_txt)\n",
    "df[\"tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spl_tokens(tweets):\n",
    "    return \"[CLS] \" + tweets + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 3)\n",
      "(566, 3)\n",
      "(453, 3)\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "test_length = int(df.shape[0] * test_size)\n",
    "test_df_bert = df.iloc[-test_length:,:]\n",
    "print(test_df_bert.shape)\n",
    "test_df_bert.reset_index(drop=True,inplace=True)\n",
    "test_df_bert.head(2)\n",
    "print(df.shape)\n",
    "train_df = df.drop(test_df_bert.index)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts_tokens = []\n",
    "txts_segment_ids = []\n",
    "indexed_tokens = []\n",
    "i=0\n",
    "max_len = 0\n",
    "max_tokens = []\n",
    "for txt in train_df[\"tweets\"]:\n",
    "    txt_cleaned = add_spl_tokens(txt)\n",
    "    txt_tokens = tokenizer.tokenize(add_spl_tokens(txt))\n",
    "    if len(txt_tokens) >512: \n",
    "        i+=1\n",
    "        if len(txt_tokens) > max_len: \n",
    "            max_len = len(txt_tokens)\n",
    "            max_tokens = txt_tokens\n",
    "        txt_tokens = txt_tokens[:512]\n",
    "    txts_tokens.append(txt_tokens)\n",
    "    txt_segment_ids = [1] * len(txt_tokens)\n",
    "    txts_segment_ids.append(txt_segment_ids)\n",
    "    txt_tokens_ids = tokenizer.convert_tokens_to_ids(txt_tokens)\n",
    "    indexed_tokens.append(txt_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documnets with more than 512 tokens:[0]\n",
      "max_len: [0]\n",
      "max_tokens: [[]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Documnets with more than 512 tokens:[{}]\".format(i))\n",
    "print(\"max_len: [{}]\".format(max_len))\n",
    "print(\"max_tokens: [{}]\".format(max_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('updated', 7172)\n",
      "('italy', 3304)\n",
      "('earthquake', 8372)\n",
      "('250', 5539)\n",
      "('fatalities', 20871)\n",
      "('360', 9475)\n",
      "('injured', 5229)\n",
      "('sis', 24761)\n",
      "('##mo', 5302)\n",
      "('te', 8915)\n",
      "('##mb', 14905)\n",
      "('##lor', 10626)\n",
      "('cnn', 13229)\n",
      "('ur', 24471)\n",
      "('##lu', 7630)\n",
      "('##rl', 12190)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "for tup in zip(txts_tokens[0], indexed_tokens[0]):\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7172,  3304,  8372,  5539, 20871,  9475,  5229, 24761,  5302,\n",
      "          8915, 14905, 10626, 13229, 24471,  7630, 12190,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens[0]])\n",
    "segments_tensors = torch.tensor([txts_segment_ids[0]])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: [12]\n",
      "Number of batches: [1]\n",
      "Number of tokens: [19]\n",
      "Number of hidden units: [768]\n"
     ]
    }
   ],
   "source": [
    "for txt_tokens_ids,txt_segment_ids in zip(indexed_tokens,txts_segment_ids):\n",
    "    tokens_tensor = torch.tensor([txt_tokens_ids])\n",
    "    segments_tensors = torch.tensor([txt_segment_ids])\n",
    "#     logger.info(tokens_tensor)\n",
    "#     logger.info(segments_tensors)\n",
    "#     logger.info(tokens_tensor.shape, segments_tensors.shape)\n",
    "    assert tokens_tensor.shape == segments_tensors.shape, \"shape does not match\"\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        \n",
    "print(\"Number of layers: [{}]\".format(len(encoded_layers)))\n",
    "layer_i = 0\n",
    "\n",
    "print(\"Number of batches: [{}]\".format(len(encoded_layers[layer_i])))\n",
    "batch_i = 0\n",
    "\n",
    "print(\"Number of tokens: [{}]\".format(len(encoded_layers[layer_i][batch_i])))\n",
    "token_i = 0\n",
    "\n",
    "print(\"Number of hidden units: [{}]\".format(len(encoded_layers[layer_i][batch_i][token_i]),\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 768])\n",
      "tensor([ 3.4558e-01, -4.3118e-01,  7.0548e-01,  4.9904e-02,  1.2772e+00,\n",
      "         5.3256e-01, -4.7265e-03,  3.7467e-01,  3.2310e-01,  8.4817e-02,\n",
      "        -4.8244e-01,  2.3197e-01,  5.8964e-01,  2.5025e-01, -1.5744e+00,\n",
      "         3.0611e-01, -9.4757e-01,  2.5359e-02, -9.3682e-01, -3.6148e-01,\n",
      "         1.1324e+00, -1.1334e-01,  5.2290e-01,  2.5939e+00,  5.9438e-02,\n",
      "        -7.1041e-02,  1.9248e+00, -2.3072e-01,  4.8047e-01, -9.4200e-01,\n",
      "         1.5071e-01, -7.0867e-01,  1.1860e+00, -1.5808e+00, -1.0279e+00,\n",
      "        -1.7174e+00, -3.4312e+00, -4.6744e-01,  1.0220e+00,  1.9994e-01,\n",
      "        -6.9383e-01,  4.8821e-01, -7.6243e-01,  4.4837e-01,  1.9844e-01,\n",
      "         5.0139e-01,  1.3200e+00, -1.6456e+00,  1.8917e-01, -6.5159e-01,\n",
      "        -6.2673e-01, -1.4725e-01,  1.6210e+00, -1.8235e-01,  9.1809e-01,\n",
      "        -1.1078e+00,  4.7874e-01, -7.1043e-01,  6.7290e-01,  4.7553e-01,\n",
      "         4.8220e-01, -8.9796e-01,  1.0826e+00, -1.4445e+00, -1.7020e+00,\n",
      "        -1.6526e-01, -7.5694e-01,  8.1939e-01,  4.2758e-02,  1.2738e+00,\n",
      "         5.6241e-01,  9.4489e-01, -6.8755e-01, -2.8413e-01, -5.3123e-01,\n",
      "        -1.4775e-01,  3.1080e-02, -6.8829e-02,  6.4084e-01, -3.6286e-01,\n",
      "        -5.9817e-01, -1.5790e+00, -4.6660e-01, -4.5329e-01,  1.0371e+00,\n",
      "         2.5552e-01, -3.3522e-01,  6.4659e-01, -7.8876e-01,  2.0102e-01,\n",
      "        -2.6625e-01,  2.9062e-01,  3.3861e-01,  8.8006e-02,  2.9098e-01,\n",
      "         3.2687e-01, -6.2930e-01,  5.4560e-02, -1.0323e+00,  9.5716e-01,\n",
      "         2.0342e+00, -8.2656e-01,  2.5622e-02, -1.4657e+00, -5.4080e-01,\n",
      "        -2.3694e-01,  5.1480e-01, -9.8465e-01, -8.2026e-02, -4.3268e-02,\n",
      "         2.0038e+00,  4.6970e-01, -5.8183e-01, -3.0704e-01,  1.9157e+00,\n",
      "         4.2542e-01, -1.1295e+00, -5.3807e-01,  2.3032e-01, -8.4461e-01,\n",
      "         1.1043e-01, -4.0166e-01, -4.6962e-01,  1.3066e+00, -1.1895e+00,\n",
      "        -3.9436e-01, -1.7256e+00, -1.0005e-01,  3.1508e-01, -3.7459e-01,\n",
      "         1.3435e+00,  1.5873e+00, -9.1131e-01, -1.1635e+00,  1.0517e+00,\n",
      "        -7.6645e-01,  1.0969e+00,  1.9750e-01, -1.7420e-01, -5.0412e-01,\n",
      "         8.5324e-01,  1.2730e-01, -2.0038e-01, -3.2499e-01,  1.8236e-01,\n",
      "        -1.9857e-01, -2.2617e+00, -3.6069e-01,  2.3150e-01,  1.8658e+00,\n",
      "        -2.5842e-01,  2.3870e-02, -1.6801e-01,  1.7887e-01, -1.6947e-01,\n",
      "        -3.7477e-02,  3.8883e-02, -7.5342e-02,  1.5623e+00,  1.5689e-01,\n",
      "         6.4951e-01, -1.1910e+00,  7.7378e-01,  1.5368e-01,  6.3558e-01,\n",
      "        -1.9919e-01,  4.7471e-01, -3.6724e-01,  4.3020e-01, -1.0549e+00,\n",
      "        -1.0424e+00, -1.1423e+00,  1.4413e-01, -3.4709e-01, -1.2508e+00,\n",
      "        -2.3138e-02,  8.6272e-01,  5.5728e-01, -4.7627e-01,  8.4901e-02,\n",
      "        -1.3964e-01,  4.8954e-01,  2.5225e-01, -9.5305e-01,  8.5430e-01,\n",
      "        -5.9526e-01,  6.1766e-01, -1.7435e-03,  9.2327e-02, -5.3824e-01,\n",
      "        -6.2071e-01,  7.3165e-02, -3.3315e-01,  2.1156e-01, -3.0751e-01,\n",
      "        -1.9758e-01, -1.3082e+00, -2.9315e-01, -7.7190e-01,  1.4651e+00,\n",
      "        -6.6622e-01,  6.2799e-01, -3.3249e-01,  8.1034e-01,  7.2077e-01,\n",
      "         1.5474e+00, -1.4352e-01, -2.0309e-01, -1.4356e+00, -5.2962e-01,\n",
      "        -1.0860e+00,  5.0975e-01, -8.2078e-01,  6.5105e-01, -7.6856e-01,\n",
      "        -8.3123e-02, -5.9247e-02, -1.0645e+00,  7.9769e-01, -1.2159e-01,\n",
      "         3.7339e-01,  1.7047e-01, -9.2507e-01,  2.2137e-02, -2.0767e+00,\n",
      "        -1.1983e-01,  1.1792e+00, -7.5388e-01, -3.9896e-01,  1.2774e+00,\n",
      "         4.5114e-01,  1.8114e-01,  1.5005e+00, -1.3845e+00,  8.0696e-01,\n",
      "         1.3156e-01, -2.2053e-01,  1.1112e-01,  4.0737e-01, -1.0890e-01,\n",
      "        -5.6424e-02,  5.8008e-01,  4.5594e-01, -1.3400e+00,  1.3546e+00,\n",
      "         5.0455e-01, -2.2118e-01,  1.3036e+00,  3.2120e-01,  3.1630e-01,\n",
      "        -1.2562e+00, -7.9265e-02,  1.3270e+00, -1.4875e-01, -4.6783e-01,\n",
      "        -6.3448e-01, -8.2266e-01, -5.8779e-01, -5.8015e-01,  5.4319e-01,\n",
      "         1.4742e+00,  2.5467e-01,  9.3069e-01, -2.2427e-01, -1.0328e+00,\n",
      "        -2.9494e-01, -3.4603e-01,  2.8411e-01,  3.3025e-01, -1.7266e+00,\n",
      "        -5.2753e-01, -1.4954e-01,  1.5912e+00, -4.1368e-01,  5.4260e-01,\n",
      "        -1.4455e+00,  2.8604e-01,  9.0948e-01, -1.7817e-02,  7.6009e-01,\n",
      "         1.3952e-01,  1.4753e+00, -1.3940e+00, -5.1021e-01,  4.6337e-01,\n",
      "         2.4684e-02, -4.5207e-01,  2.0244e+00, -7.6071e-01,  1.9322e+00,\n",
      "        -1.9178e-01, -7.0636e-01,  6.7786e-01,  6.2360e-01,  1.2658e+00,\n",
      "         7.2522e-02, -5.8760e-01,  3.2544e-01,  1.0899e+00,  2.1836e-01,\n",
      "         8.7066e-01,  1.2892e-01,  3.5698e-01,  1.4092e-01, -7.1688e-01,\n",
      "         9.0797e-01,  2.3843e-01, -9.1834e-01, -1.2030e+00, -8.1326e-01,\n",
      "         9.8317e-01, -6.4142e-01,  4.9691e-01,  8.5995e-01,  4.0620e-01,\n",
      "         3.7505e-03, -4.7480e-01, -4.8392e-01,  5.8019e-01,  5.0285e-01,\n",
      "        -3.5289e-01, -5.5790e-01,  7.7654e-01, -1.0628e+00,  5.4396e-02,\n",
      "         8.8863e-01, -9.0742e-01, -8.6169e-01, -1.9867e-01,  1.1130e+00,\n",
      "        -3.7688e-01, -3.5739e-02,  2.0979e-01,  7.9533e-01,  2.4762e-02,\n",
      "         6.2835e-01,  2.1916e-01, -4.6653e-01,  4.7430e-01, -7.5350e-01,\n",
      "         7.2589e-01,  1.1534e+00,  3.9833e-01,  9.8863e-01, -5.3614e-01,\n",
      "        -4.9387e-01, -9.0612e-01,  1.1184e-01, -7.5002e-01, -9.3063e-01,\n",
      "        -1.2489e+00, -1.6178e+00,  2.2673e-01, -4.6527e-02, -6.9292e-01,\n",
      "         7.9545e-01, -1.0355e+00, -8.4297e-01,  4.5770e-01, -6.2417e-01,\n",
      "        -9.1092e-02,  3.2867e-01, -5.0965e-01, -1.7292e-01,  5.0435e-01,\n",
      "        -5.5013e-01, -1.7083e-01, -1.7390e-01, -6.4370e-01, -2.6588e-01,\n",
      "        -1.3705e+00,  3.3856e-01,  9.6365e-02,  7.5800e-02, -1.7803e+00,\n",
      "         8.3495e-01,  4.2464e-01, -2.1940e+00, -1.1290e-01, -1.3429e-01,\n",
      "        -6.0053e-01, -2.3966e+00, -1.2867e-01, -2.4877e-01, -4.4692e-01,\n",
      "        -3.5805e-01, -5.3021e-01,  6.2230e-01, -5.9709e-01,  7.2851e-01,\n",
      "         1.0039e+00,  7.6059e-01,  2.0986e-03, -2.4898e-01, -2.9524e-01,\n",
      "        -1.1066e+00, -8.1259e-01, -3.4882e-01, -1.2561e-01, -2.1815e-01,\n",
      "         8.1117e-01,  3.4024e-01, -1.2146e-01, -1.8115e+00, -2.1990e-01,\n",
      "         1.9276e-01,  1.2816e+00, -3.3574e-01,  9.3586e-01, -7.9615e-01,\n",
      "        -4.2819e-01, -1.7059e-01, -8.6994e-01,  8.2633e-01, -5.8920e-01,\n",
      "         6.0541e-01, -3.7447e-01, -1.1051e+00,  2.9324e-02,  1.7813e-01,\n",
      "        -4.4309e-01, -1.3429e+00,  1.2931e+00, -9.7743e-01,  6.3765e-01,\n",
      "         3.9799e-01,  3.6580e-01, -8.8897e-01, -1.7133e+00, -1.4484e-01,\n",
      "         1.5730e+00,  1.0802e-01, -2.8213e-01, -3.1361e-01, -9.3347e-01,\n",
      "         3.2333e-01, -7.4666e-02,  2.5454e-01,  7.4118e-01,  3.8962e-01,\n",
      "         5.3276e-01, -1.1637e+00,  2.4251e-01,  8.0195e-01, -2.2867e-01,\n",
      "        -1.0485e+00, -2.3600e-01, -2.7061e-01,  1.2448e+00, -5.6214e-01,\n",
      "         1.9005e-01,  2.3060e+00, -8.7339e-01,  8.6976e-02, -1.1044e+00,\n",
      "        -2.9376e-01, -4.2541e-01,  5.0586e-01,  3.1644e-01, -1.7826e+00,\n",
      "         1.9487e-01,  6.8427e-02, -6.8345e-01,  1.2834e-01,  4.6741e-01,\n",
      "        -3.4228e-01, -1.2344e+00, -1.1041e+00, -9.7306e-01, -6.0877e-01,\n",
      "         2.1520e-01,  8.5490e-01, -8.8626e-03,  1.0402e+00,  4.5150e-01,\n",
      "        -5.6388e-02,  2.7178e-01, -5.2750e-01,  2.2008e-01,  5.0176e-01,\n",
      "         8.2897e-01, -9.6596e-01, -1.1233e+00, -9.9192e-01,  1.3271e+00,\n",
      "         4.9486e-02,  5.0653e-01,  2.9636e-01,  8.7958e-01,  9.1911e-02,\n",
      "         1.5183e-02, -9.3466e-02,  4.3673e-01, -9.0818e-01,  3.1677e-01,\n",
      "         8.4004e-01,  5.3746e-01,  3.4482e-01,  7.7157e-02,  1.5116e+00,\n",
      "        -1.8284e+00, -3.4189e-01,  8.1292e-01,  7.9661e-01, -3.8378e-01,\n",
      "        -9.2778e-01, -8.3451e-01,  6.2215e-01, -3.8146e-01, -1.2177e+00,\n",
      "        -4.5329e-01, -2.9886e-01, -2.0895e+00, -6.2046e-02, -4.0683e-01,\n",
      "        -3.4217e-01,  1.8908e+00, -5.1761e-01,  1.8826e-01,  1.1633e+00,\n",
      "         3.4613e-01,  6.7039e-01, -5.3083e-02,  5.2576e-01,  3.8701e-01,\n",
      "         9.4120e-01,  4.4715e-01, -1.4446e+00,  3.9299e-02, -2.2343e-01,\n",
      "        -9.4157e-02, -1.7214e+00, -4.9914e-02,  5.7084e-01,  1.1277e+00,\n",
      "        -1.8619e+00, -5.5091e-01,  2.6243e-01,  8.7948e-01, -2.7787e-01,\n",
      "         1.0645e+00,  4.6864e-01,  3.6049e-01, -2.0342e-01, -5.2737e-01,\n",
      "        -3.5985e-02,  6.8894e-01,  7.3151e-01,  4.3340e-01, -5.2478e-01,\n",
      "        -1.8316e+00,  5.1319e-01,  2.8824e-01,  1.2600e+00, -9.5214e-01,\n",
      "        -3.9269e-01,  1.7725e-01,  1.0405e+00,  1.3092e+00,  2.1221e-01,\n",
      "         6.8280e-01, -6.1478e-02, -1.0413e+00, -5.8757e-01, -2.1007e-02,\n",
      "        -1.0210e-01, -3.4161e-01, -2.3847e-01, -1.3580e+00, -2.0026e+00,\n",
      "         9.6605e-01,  1.7422e-01,  2.9425e-01,  1.1926e-01, -2.8455e-02,\n",
      "        -1.4551e+00,  2.2849e-01,  2.4868e-01, -5.4338e-01, -4.9869e-01,\n",
      "        -1.2097e-01,  5.0639e-01, -1.1065e+00, -2.9974e-01, -1.0704e+00,\n",
      "        -3.3123e-01,  1.0030e+00,  9.6929e-01,  2.0889e-01, -4.7850e-01,\n",
      "        -9.4300e-01,  5.7807e-01,  5.8972e-01,  1.4701e+00,  8.9679e-02,\n",
      "        -6.2108e-01,  6.6906e-01, -5.6397e-01,  1.5531e+00,  6.1625e-01,\n",
      "        -3.6593e-01,  1.3120e+00,  7.4210e-01,  4.2808e-01,  1.4650e-01,\n",
      "         1.6606e+00,  2.9380e-01,  6.7947e-01,  5.0702e-01,  6.8292e-02,\n",
      "         1.9167e-01, -7.8623e-02,  7.2833e-01,  2.8255e-01, -8.8114e-01,\n",
      "        -1.0442e+00, -1.4894e+00,  5.6420e-01, -1.8692e+00,  1.6038e-01,\n",
      "        -1.2224e-01,  4.9681e-01,  1.2262e+00,  9.1608e-02, -1.6390e-01,\n",
      "        -1.5016e-01,  2.7804e-01, -3.5151e-01, -7.4843e-01,  1.9901e-01,\n",
      "         1.2614e+00,  3.3566e-01, -1.5662e-01, -4.7350e-01,  2.8675e-01,\n",
      "         3.0885e-03,  1.8689e+00,  5.6483e-01, -3.9653e-01,  6.0832e-01,\n",
      "         6.4776e-01,  5.1908e-01,  3.2852e-01,  4.6449e-01,  5.3960e-01,\n",
      "        -3.1487e-01,  1.5472e-01, -1.4715e+00, -3.8635e-01,  1.1804e-01,\n",
      "        -3.2123e-01, -5.4119e-01, -1.9981e-01, -1.4945e-01, -4.8821e-01,\n",
      "         4.6941e-01, -6.9749e-01, -6.0272e-01, -1.6371e+00, -1.2891e+00,\n",
      "         5.3837e-01, -4.2008e-02, -1.5130e-01, -8.3839e-01,  4.9309e-01,\n",
      "         7.9196e-01, -4.4927e-01,  2.5436e-01,  3.4403e-01,  2.9703e-01,\n",
      "        -7.1792e-01, -1.0326e+00, -1.4338e+00, -3.2119e-01,  5.9351e-01,\n",
      "         2.3503e-01,  1.4083e+00,  7.9844e-01, -4.6833e-01,  1.1893e-01,\n",
      "        -6.1392e-01,  1.4712e+00,  2.9519e-01,  5.8080e-01, -7.8009e-01,\n",
      "        -9.2329e-02,  2.6992e-01, -1.5226e+00, -3.9656e-01,  1.4497e-01,\n",
      "         8.9154e-01,  9.0570e-01,  6.4647e-01,  5.7883e-01, -9.4463e-01,\n",
      "        -1.9449e-01, -2.1178e-01,  9.9460e-01,  2.8974e-01, -6.2693e-02,\n",
      "         1.7287e-01, -1.6163e+00,  3.1879e-01,  8.4365e-01,  8.8981e-02,\n",
      "        -3.5607e-01, -7.9129e-01, -1.3776e+00,  1.6899e+00, -1.4439e+00,\n",
      "        -1.7005e-01, -8.9752e-01,  4.3951e-01,  7.4737e-01, -1.0072e+00,\n",
      "         1.7932e+00, -1.4132e-01, -1.2479e+00,  5.5917e-01, -1.9889e+00,\n",
      "         2.0831e-01,  1.0190e+00, -2.6812e-01, -3.0947e-01,  1.0040e+00,\n",
      "         7.1170e-01, -8.2119e-02,  2.2698e-01, -8.3504e-01, -5.3787e-01,\n",
      "         8.0026e-01,  6.4409e-01,  2.5766e-01,  1.6808e+00,  6.0061e-01,\n",
      "         4.8347e-01, -2.9294e-01, -5.7364e-01, -1.3336e+00,  1.5782e-01,\n",
      "        -7.0206e-01, -3.0835e-01,  1.2387e+00, -2.2267e+00,  3.1173e-01,\n",
      "         5.5410e-01, -1.0732e+00,  4.6891e-01,  1.2827e+00, -8.1666e-01,\n",
      "         8.9335e-02,  7.1236e-01, -2.5700e-01, -1.2755e+00,  5.2509e-01,\n",
      "        -5.3735e-02, -1.2826e+00,  1.3789e+00,  7.0047e-01, -1.7012e+00,\n",
      "        -5.8033e-01, -2.4945e-01, -1.2366e+00, -1.5201e-02, -8.2278e-01,\n",
      "        -7.9087e-01, -3.5030e-01,  3.8929e-01])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAI/CAYAAAC4QOfKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_i = 6\n",
    "batch_i = 0\n",
    "layer_i = 6\n",
    "\n",
    "print(encoded_layers[layer_i][0].shape)\n",
    "\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "print(vec)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "['[CLS]', 'survival', 'wild', 'survival', 'tool', 'aid', 'touch', 'emergency', 'bag', 'earthquake', 'aid', 'bag', 'ur', '##lu', '##rl', 'ur', '##lu', '##rl', '[SEP]']\n",
      "453\n",
      "18\n",
      "0 0 0\n",
      "torch.Size([19, 768])\n",
      "1 0 0\n",
      "torch.Size([19, 768])\n",
      "2 0 0\n",
      "torch.Size([19, 768])\n",
      "3 0 0\n",
      "torch.Size([19, 768])\n",
      "4 0 0\n",
      "torch.Size([19, 768])\n",
      "5 0 0\n",
      "torch.Size([19, 768])\n",
      "6 0 0\n",
      "torch.Size([19, 768])\n",
      "7 0 0\n",
      "torch.Size([19, 768])\n",
      "8 0 0\n",
      "torch.Size([19, 768])\n",
      "9 0 0\n",
      "torch.Size([19, 768])\n",
      "10 0 0\n",
      "torch.Size([19, 768])\n",
      "11 0 0\n",
      "torch.Size([19, 768])\n",
      "0 0 1\n",
      "torch.Size([19, 768])\n",
      "1 0 1\n",
      "torch.Size([19, 768])\n",
      "2 0 1\n",
      "torch.Size([19, 768])\n",
      "3 0 1\n",
      "torch.Size([19, 768])\n",
      "4 0 1\n",
      "torch.Size([19, 768])\n",
      "5 0 1\n",
      "torch.Size([19, 768])\n",
      "6 0 1\n",
      "torch.Size([19, 768])\n",
      "7 0 1\n",
      "torch.Size([19, 768])\n",
      "8 0 1\n",
      "torch.Size([19, 768])\n",
      "9 0 1\n",
      "torch.Size([19, 768])\n",
      "10 0 1\n",
      "torch.Size([19, 768])\n",
      "11 0 1\n",
      "torch.Size([19, 768])\n",
      "0 0 2\n",
      "torch.Size([19, 768])\n",
      "1 0 2\n",
      "torch.Size([19, 768])\n",
      "2 0 2\n",
      "torch.Size([19, 768])\n",
      "3 0 2\n",
      "torch.Size([19, 768])\n",
      "4 0 2\n",
      "torch.Size([19, 768])\n",
      "5 0 2\n",
      "torch.Size([19, 768])\n",
      "6 0 2\n",
      "torch.Size([19, 768])\n",
      "7 0 2\n",
      "torch.Size([19, 768])\n",
      "8 0 2\n",
      "torch.Size([19, 768])\n",
      "9 0 2\n",
      "torch.Size([19, 768])\n",
      "10 0 2\n",
      "torch.Size([19, 768])\n",
      "11 0 2\n",
      "torch.Size([19, 768])\n",
      "0 0 3\n",
      "torch.Size([19, 768])\n",
      "1 0 3\n",
      "torch.Size([19, 768])\n",
      "2 0 3\n",
      "torch.Size([19, 768])\n",
      "3 0 3\n",
      "torch.Size([19, 768])\n",
      "4 0 3\n",
      "torch.Size([19, 768])\n",
      "5 0 3\n",
      "torch.Size([19, 768])\n",
      "6 0 3\n",
      "torch.Size([19, 768])\n",
      "7 0 3\n",
      "torch.Size([19, 768])\n",
      "8 0 3\n",
      "torch.Size([19, 768])\n",
      "9 0 3\n",
      "torch.Size([19, 768])\n",
      "10 0 3\n",
      "torch.Size([19, 768])\n",
      "11 0 3\n",
      "torch.Size([19, 768])\n",
      "0 0 4\n",
      "torch.Size([19, 768])\n",
      "1 0 4\n",
      "torch.Size([19, 768])\n",
      "2 0 4\n",
      "torch.Size([19, 768])\n",
      "3 0 4\n",
      "torch.Size([19, 768])\n",
      "4 0 4\n",
      "torch.Size([19, 768])\n",
      "5 0 4\n",
      "torch.Size([19, 768])\n",
      "6 0 4\n",
      "torch.Size([19, 768])\n",
      "7 0 4\n",
      "torch.Size([19, 768])\n",
      "8 0 4\n",
      "torch.Size([19, 768])\n",
      "9 0 4\n",
      "torch.Size([19, 768])\n",
      "10 0 4\n",
      "torch.Size([19, 768])\n",
      "11 0 4\n",
      "torch.Size([19, 768])\n",
      "0 0 5\n",
      "torch.Size([19, 768])\n",
      "1 0 5\n",
      "torch.Size([19, 768])\n",
      "2 0 5\n",
      "torch.Size([19, 768])\n",
      "3 0 5\n",
      "torch.Size([19, 768])\n",
      "4 0 5\n",
      "torch.Size([19, 768])\n",
      "5 0 5\n",
      "torch.Size([19, 768])\n",
      "6 0 5\n",
      "torch.Size([19, 768])\n",
      "7 0 5\n",
      "torch.Size([19, 768])\n",
      "8 0 5\n",
      "torch.Size([19, 768])\n",
      "9 0 5\n",
      "torch.Size([19, 768])\n",
      "10 0 5\n",
      "torch.Size([19, 768])\n",
      "11 0 5\n",
      "torch.Size([19, 768])\n",
      "0 0 6\n",
      "torch.Size([19, 768])\n",
      "1 0 6\n",
      "torch.Size([19, 768])\n",
      "2 0 6\n",
      "torch.Size([19, 768])\n",
      "3 0 6\n",
      "torch.Size([19, 768])\n",
      "4 0 6\n",
      "torch.Size([19, 768])\n",
      "5 0 6\n",
      "torch.Size([19, 768])\n",
      "6 0 6\n",
      "torch.Size([19, 768])\n",
      "7 0 6\n",
      "torch.Size([19, 768])\n",
      "8 0 6\n",
      "torch.Size([19, 768])\n",
      "9 0 6\n",
      "torch.Size([19, 768])\n",
      "10 0 6\n",
      "torch.Size([19, 768])\n",
      "11 0 6\n",
      "torch.Size([19, 768])\n",
      "0 0 7\n",
      "torch.Size([19, 768])\n",
      "1 0 7\n",
      "torch.Size([19, 768])\n",
      "2 0 7\n",
      "torch.Size([19, 768])\n",
      "3 0 7\n",
      "torch.Size([19, 768])\n",
      "4 0 7\n",
      "torch.Size([19, 768])\n",
      "5 0 7\n",
      "torch.Size([19, 768])\n",
      "6 0 7\n",
      "torch.Size([19, 768])\n",
      "7 0 7\n",
      "torch.Size([19, 768])\n",
      "8 0 7\n",
      "torch.Size([19, 768])\n",
      "9 0 7\n",
      "torch.Size([19, 768])\n",
      "10 0 7\n",
      "torch.Size([19, 768])\n",
      "11 0 7\n",
      "torch.Size([19, 768])\n",
      "0 0 8\n",
      "torch.Size([19, 768])\n",
      "1 0 8\n",
      "torch.Size([19, 768])\n",
      "2 0 8\n",
      "torch.Size([19, 768])\n",
      "3 0 8\n",
      "torch.Size([19, 768])\n",
      "4 0 8\n",
      "torch.Size([19, 768])\n",
      "5 0 8\n",
      "torch.Size([19, 768])\n",
      "6 0 8\n",
      "torch.Size([19, 768])\n",
      "7 0 8\n",
      "torch.Size([19, 768])\n",
      "8 0 8\n",
      "torch.Size([19, 768])\n",
      "9 0 8\n",
      "torch.Size([19, 768])\n",
      "10 0 8\n",
      "torch.Size([19, 768])\n",
      "11 0 8\n",
      "torch.Size([19, 768])\n",
      "0 0 9\n",
      "torch.Size([19, 768])\n",
      "1 0 9\n",
      "torch.Size([19, 768])\n",
      "2 0 9\n",
      "torch.Size([19, 768])\n",
      "3 0 9\n",
      "torch.Size([19, 768])\n",
      "4 0 9\n",
      "torch.Size([19, 768])\n",
      "5 0 9\n",
      "torch.Size([19, 768])\n",
      "6 0 9\n",
      "torch.Size([19, 768])\n",
      "7 0 9\n",
      "torch.Size([19, 768])\n",
      "8 0 9\n",
      "torch.Size([19, 768])\n",
      "9 0 9\n",
      "torch.Size([19, 768])\n",
      "10 0 9\n",
      "torch.Size([19, 768])\n",
      "11 0 9\n",
      "torch.Size([19, 768])\n",
      "0 0 10\n",
      "torch.Size([19, 768])\n",
      "1 0 10\n",
      "torch.Size([19, 768])\n",
      "2 0 10\n",
      "torch.Size([19, 768])\n",
      "3 0 10\n",
      "torch.Size([19, 768])\n",
      "4 0 10\n",
      "torch.Size([19, 768])\n",
      "5 0 10\n",
      "torch.Size([19, 768])\n",
      "6 0 10\n",
      "torch.Size([19, 768])\n",
      "7 0 10\n",
      "torch.Size([19, 768])\n",
      "8 0 10\n",
      "torch.Size([19, 768])\n",
      "9 0 10\n",
      "torch.Size([19, 768])\n",
      "10 0 10\n",
      "torch.Size([19, 768])\n",
      "11 0 10\n",
      "torch.Size([19, 768])\n",
      "0 0 11\n",
      "torch.Size([19, 768])\n",
      "1 0 11\n",
      "torch.Size([19, 768])\n",
      "2 0 11\n",
      "torch.Size([19, 768])\n",
      "3 0 11\n",
      "torch.Size([19, 768])\n",
      "4 0 11\n",
      "torch.Size([19, 768])\n",
      "5 0 11\n",
      "torch.Size([19, 768])\n",
      "6 0 11\n",
      "torch.Size([19, 768])\n",
      "7 0 11\n",
      "torch.Size([19, 768])\n",
      "8 0 11\n",
      "torch.Size([19, 768])\n",
      "9 0 11\n",
      "torch.Size([19, 768])\n",
      "10 0 11\n",
      "torch.Size([19, 768])\n",
      "11 0 11\n",
      "torch.Size([19, 768])\n",
      "0 0 12\n",
      "torch.Size([19, 768])\n",
      "1 0 12\n",
      "torch.Size([19, 768])\n",
      "2 0 12\n",
      "torch.Size([19, 768])\n",
      "3 0 12\n",
      "torch.Size([19, 768])\n",
      "4 0 12\n",
      "torch.Size([19, 768])\n",
      "5 0 12\n",
      "torch.Size([19, 768])\n",
      "6 0 12\n",
      "torch.Size([19, 768])\n",
      "7 0 12\n",
      "torch.Size([19, 768])\n",
      "8 0 12\n",
      "torch.Size([19, 768])\n",
      "9 0 12\n",
      "torch.Size([19, 768])\n",
      "10 0 12\n",
      "torch.Size([19, 768])\n",
      "11 0 12\n",
      "torch.Size([19, 768])\n",
      "0 0 13\n",
      "torch.Size([19, 768])\n",
      "1 0 13\n",
      "torch.Size([19, 768])\n",
      "2 0 13\n",
      "torch.Size([19, 768])\n",
      "3 0 13\n",
      "torch.Size([19, 768])\n",
      "4 0 13\n",
      "torch.Size([19, 768])\n",
      "5 0 13\n",
      "torch.Size([19, 768])\n",
      "6 0 13\n",
      "torch.Size([19, 768])\n",
      "7 0 13\n",
      "torch.Size([19, 768])\n",
      "8 0 13\n",
      "torch.Size([19, 768])\n",
      "9 0 13\n",
      "torch.Size([19, 768])\n",
      "10 0 13\n",
      "torch.Size([19, 768])\n",
      "11 0 13\n",
      "torch.Size([19, 768])\n",
      "0 0 14\n",
      "torch.Size([19, 768])\n",
      "1 0 14\n",
      "torch.Size([19, 768])\n",
      "2 0 14\n",
      "torch.Size([19, 768])\n",
      "3 0 14\n",
      "torch.Size([19, 768])\n",
      "4 0 14\n",
      "torch.Size([19, 768])\n",
      "5 0 14\n",
      "torch.Size([19, 768])\n",
      "6 0 14\n",
      "torch.Size([19, 768])\n",
      "7 0 14\n",
      "torch.Size([19, 768])\n",
      "8 0 14\n",
      "torch.Size([19, 768])\n",
      "9 0 14\n",
      "torch.Size([19, 768])\n",
      "10 0 14\n",
      "torch.Size([19, 768])\n",
      "11 0 14\n",
      "torch.Size([19, 768])\n",
      "0 0 15\n",
      "torch.Size([19, 768])\n",
      "1 0 15\n",
      "torch.Size([19, 768])\n",
      "2 0 15\n",
      "torch.Size([19, 768])\n",
      "3 0 15\n",
      "torch.Size([19, 768])\n",
      "4 0 15\n",
      "torch.Size([19, 768])\n",
      "5 0 15\n",
      "torch.Size([19, 768])\n",
      "6 0 15\n",
      "torch.Size([19, 768])\n",
      "7 0 15\n",
      "torch.Size([19, 768])\n",
      "8 0 15\n",
      "torch.Size([19, 768])\n",
      "9 0 15\n",
      "torch.Size([19, 768])\n",
      "10 0 15\n",
      "torch.Size([19, 768])\n",
      "11 0 15\n",
      "torch.Size([19, 768])\n",
      "0 0 16\n",
      "torch.Size([19, 768])\n",
      "1 0 16\n",
      "torch.Size([19, 768])\n",
      "2 0 16\n",
      "torch.Size([19, 768])\n",
      "3 0 16\n",
      "torch.Size([19, 768])\n",
      "4 0 16\n",
      "torch.Size([19, 768])\n",
      "5 0 16\n",
      "torch.Size([19, 768])\n",
      "6 0 16\n",
      "torch.Size([19, 768])\n",
      "7 0 16\n",
      "torch.Size([19, 768])\n",
      "8 0 16\n",
      "torch.Size([19, 768])\n",
      "9 0 16\n",
      "torch.Size([19, 768])\n",
      "10 0 16\n",
      "torch.Size([19, 768])\n",
      "11 0 16\n",
      "torch.Size([19, 768])\n",
      "0 0 17\n",
      "torch.Size([19, 768])\n",
      "1 0 17\n",
      "torch.Size([19, 768])\n",
      "2 0 17\n",
      "torch.Size([19, 768])\n",
      "3 0 17\n",
      "torch.Size([19, 768])\n",
      "4 0 17\n",
      "torch.Size([19, 768])\n",
      "5 0 17\n",
      "torch.Size([19, 768])\n",
      "6 0 17\n",
      "torch.Size([19, 768])\n",
      "7 0 17\n",
      "torch.Size([19, 768])\n",
      "8 0 17\n",
      "torch.Size([19, 768])\n",
      "9 0 17\n",
      "torch.Size([19, 768])\n",
      "10 0 17\n",
      "torch.Size([19, 768])\n",
      "11 0 17\n",
      "torch.Size([19, 768])\n",
      "0 0 18\n",
      "torch.Size([19, 768])\n",
      "1 0 18\n",
      "torch.Size([19, 768])\n",
      "2 0 18\n",
      "torch.Size([19, 768])\n",
      "3 0 18\n",
      "torch.Size([19, 768])\n",
      "4 0 18\n",
      "torch.Size([19, 768])\n",
      "5 0 18\n",
      "torch.Size([19, 768])\n",
      "6 0 18\n",
      "torch.Size([19, 768])\n",
      "7 0 18\n",
      "torch.Size([19, 768])\n",
      "8 0 18\n",
      "torch.Size([19, 768])\n",
      "9 0 18\n",
      "torch.Size([19, 768])\n",
      "10 0 18\n",
      "torch.Size([19, 768])\n",
      "11 0 18\n",
      "torch.Size([19, 768])\n",
      "0 0 19\n",
      "torch.Size([19, 768])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 19 is out of bounds for dimension 0 with size 19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-1ab91544cb4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mhidden_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 19 is out of bounds for dimension 0 with size 19"
     ]
    }
   ],
   "source": [
    "print(len(txts_tokens[1]))\n",
    "print(txts_tokens[1])\n",
    "print(len(txts_tokens))\n",
    "\n",
    "for txt_tokens in txts_tokens:\n",
    "    token_embeddings = []\n",
    "    print(len(txt_tokens))\n",
    "    for token_i in range(len(txts_tokens)):\n",
    "        hidden_layers = [] \n",
    "        for layer_i in range(len(encoded_layers)):\n",
    "            print(layer_i,batch_i,token_i)\n",
    "            print(encoded_layers[layer_i][batch_i].shape)\n",
    "            vec = encoded_layers[layer_i][batch_i][token_i]    \n",
    "            hidden_layers.append(vec)\n",
    "        token_embeddings.append(hidden_layers)\n",
    "\n",
    "# Sanity check the dimensions:\n",
    "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "print (\"Number of layers per token:\", len(token_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs_cat = []\n",
    "token_vecs_sum = []\n",
    "\n",
    "for token in token_embeddings:\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), 0)\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "    sum_vec = torch.sum(torch.stack(token)[-4:], 0)\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print('Concatenated vector shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "print('Sum vector shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(encoded_layers[11], 1)\n",
    "print (\"Our final sentence embedding vector of shape:\"), sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def knn_lbl_majority(test_features,train_features,train_labels,k=5):\n",
    "\n",
    "#     from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#     NN = KNeighborsClassifier(n_neighbors=k)\n",
    "#     NN.fit(train_features,train_labels)\n",
    "#     test_labels = NN.predict(test_features)\n",
    "#     ## To get probabilities: test_labels_probas = NN.predict_proba(test_features)\n",
    "#     return test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighrest_neighbors(test_features,train_features,n,k=2):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    NN = NearestNeighbors(n_neighbors=k)\n",
    "    NN.fit(train_features)\n",
    "    closest_neighbors_from_train = NN.kneighbors(test_features,\n",
    "    return_distance=False)\n",
    "    return closest_neighbors_from_train[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_features = test_df['tweets']\n",
    "train_features = txts_token['tweets']\n",
    "train_labels = txts_token['labels']\n",
    "n=2\n",
    "n_idxs = neighrest_neighbors(test_features, train_features, n)\n",
    "count = np.zeros(4)\n",
    "#n is for precision at k\n",
    "print(test_features)\n",
    "print(train_features)\n",
    "print(train_labels)\n",
    "for ii in n_idxs:\n",
    "    count = np.zeros(4)\n",
    "    for l in range(0,n):\n",
    "        for i in n_idxs[l]:\n",
    "            for j in train_labels[i]:\n",
    "                count[j] = count[j] + 1\n",
    "\n",
    "    \n",
    "    print(np.argsort(count)[4-n:])\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "# print(test_labels[0:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
